{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from MIMIC csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3524: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ndc2rxnorm_mapping.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sz/kp0vj2ld573b7ghvrbmspgx80000gn/T/ipykernel_59037/2480275691.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_final.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/sz/kp0vj2ld573b7ghvrbmspgx80000gn/T/ipykernel_59037/2480275691.py\u001b[0m in \u001b[0;36mprocess_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# get med and diag (visit>=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mmed_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_med\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmed_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndc2atc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmed_pd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;31m#     med_pd = filter_300_most_med(med_pd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/sz/kp0vj2ld573b7ghvrbmspgx80000gn/T/ipykernel_59037/2480275691.py\u001b[0m in \u001b[0;36mndc2atc4\u001b[0;34m(med_pd)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mndc2atc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmed_pd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndc_rxnorm_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mndc2rxnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mmed_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RXCUI'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmed_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NDC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndc2rxnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ndc2rxnorm_mapping.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# files can be downloaded from https://mimic.physionet.org/gettingstarted/dbsetup/\n",
    "med_file = 'PRESCRIPTIONS.csv'\n",
    "diag_file = 'DIAGNOSES_ICD.csv'\n",
    "procedure_file = 'PROCEDURES_ICD.csv'\n",
    "\n",
    "# drug code mapping files (already in ./data/)\n",
    "ndc2atc_file = 'ndc2atc_level4.csv' \n",
    "cid_atc = 'drug-atc.csv'\n",
    "ndc_rxnorm_file = 'ndc2rxnorm_mapping.txt'\n",
    "\n",
    "# drug-drug interactions can be down https://www.dropbox.com/s/8os4pd2zmp2jemd/drug-DDI.csv?dl=0\n",
    "ddi_file = 'drug-DDI.csv'\n",
    "\n",
    "def process_procedure():\n",
    "    pro_pd = pd.read_csv(procedure_file, dtype={'ICD9_CODE':'category'})\n",
    "    pro_pd.drop(columns=['ROW_ID'], inplace=True)\n",
    "#     pro_pd = pro_pd[pro_pd['SEQ_NUM']<5]\n",
    "#     def icd9_tree(x):\n",
    "#         if x[0]=='E':\n",
    "#             return x[:4] \n",
    "#         return x[:3]\n",
    "#     pro_pd['ICD9_CODE'] = pro_pd['ICD9_CODE'].map(icd9_tree)\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM'], inplace=True)\n",
    "    pro_pd.drop(columns=['SEQ_NUM'], inplace=True)\n",
    "    pro_pd.drop_duplicates(inplace=True)\n",
    "    pro_pd.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return pro_pd\n",
    "\n",
    "\n",
    "def process_med():\n",
    "    med_pd = pd.read_csv(med_file, dtype={'NDC':'category'})\n",
    "    # filter\n",
    "    med_pd.drop(columns=['ROW_ID','DRUG_TYPE','DRUG_NAME_POE','DRUG_NAME_GENERIC',\n",
    "                     'FORMULARY_DRUG_CD','GSN','PROD_STRENGTH','DOSE_VAL_RX',\n",
    "                     'DOSE_UNIT_RX','FORM_VAL_DISP','FORM_UNIT_DISP','FORM_UNIT_DISP',\n",
    "                      'ROUTE','ENDDATE','DRUG'], axis=1, inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['NDC'] == '0'].index, axis=0, inplace=True)\n",
    "    med_pd.fillna(method='pad', inplace=True)\n",
    "    med_pd.dropna(inplace=True)\n",
    "    med_pd.drop_duplicates(inplace=True)\n",
    "    med_pd['ICUSTAY_ID'] = med_pd['ICUSTAY_ID'].astype('int64')\n",
    "    med_pd['STARTDATE'] = pd.to_datetime(med_pd['STARTDATE'], format='%Y-%m-%d %H:%M:%S')    \n",
    "    med_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTDATE'], inplace=True)\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    \n",
    "    def filter_first24hour_med(med_pd):\n",
    "        med_pd_new = med_pd.drop(columns=['NDC'])\n",
    "        med_pd_new = med_pd_new.groupby(by=['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).head([1]).reset_index(drop=True)\n",
    "        med_pd_new = pd.merge(med_pd_new, med_pd, on=['SUBJECT_ID','HADM_ID','ICUSTAY_ID','STARTDATE'])\n",
    "        med_pd_new = med_pd_new.drop(columns=['STARTDATE'])\n",
    "        return med_pd_new\n",
    "    med_pd = filter_first24hour_med(med_pd)\n",
    "#     med_pd = med_pd.drop(columns=['STARTDATE'])\n",
    "    \n",
    "    med_pd = med_pd.drop(columns=['ICUSTAY_ID'])\n",
    "    med_pd = med_pd.drop_duplicates()\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    \n",
    "    # visit > 2\n",
    "    def process_visit_lg2(med_pd):\n",
    "        a = med_pd[['SUBJECT_ID', 'HADM_ID']].groupby(by='SUBJECT_ID')['HADM_ID'].unique().reset_index()\n",
    "        a['HADM_ID_Len'] = a['HADM_ID'].map(lambda x:len(x))\n",
    "        a = a[a['HADM_ID_Len'] > 1]\n",
    "        return a \n",
    "    med_pd_lg2 = process_visit_lg2(med_pd).reset_index(drop=True)    \n",
    "    med_pd = med_pd.merge(med_pd_lg2[['SUBJECT_ID']], on='SUBJECT_ID', how='inner')    \n",
    "    \n",
    "    return med_pd.reset_index(drop=True)\n",
    "\n",
    "def process_diag():\n",
    "    diag_pd = pd.read_csv(diag_file)\n",
    "    diag_pd.dropna(inplace=True)\n",
    "#     def icd9_tree(x):\n",
    "#         if x[0]=='E':\n",
    "#             return x[:4] \n",
    "#         return x[:3]\n",
    "#     diag_pd['ICD9_CODE'] = diag_pd['ICD9_CODE'].map(icd9_tree)\n",
    "#     diag_pd = diag_pd[diag_pd['SEQ_NUM'] < 5]\n",
    "    diag_pd.drop(columns=['SEQ_NUM','ROW_ID'],inplace=True)\n",
    "    diag_pd.drop_duplicates(inplace=True)\n",
    "    diag_pd.sort_values(by=['SUBJECT_ID','HADM_ID'], inplace=True)\n",
    "    return diag_pd.reset_index(drop=True)\n",
    "\n",
    "def ndc2atc4(med_pd):\n",
    "    with open(ndc_rxnorm_file, 'r') as f:\n",
    "        ndc2rxnorm = eval(f.read())\n",
    "    med_pd['RXCUI'] = med_pd['NDC'].map(ndc2rxnorm)\n",
    "    med_pd.dropna(inplace=True)\n",
    "\n",
    "    rxnorm2atc = pd.read_csv(ndc2atc_file)\n",
    "    rxnorm2atc = rxnorm2atc.drop(columns=['YEAR','MONTH','NDC'])\n",
    "    rxnorm2atc.drop_duplicates(subset=['RXCUI'], inplace=True)\n",
    "    med_pd.drop(index = med_pd[med_pd['RXCUI'].isin([''])].index, axis=0, inplace=True)\n",
    "    \n",
    "    med_pd['RXCUI'] = med_pd['RXCUI'].astype('int64')\n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    med_pd = med_pd.merge(rxnorm2atc, on=['RXCUI'])\n",
    "    med_pd.drop(columns=['NDC', 'RXCUI'], inplace=True)\n",
    "    med_pd = med_pd.rename(columns={'ATC4':'NDC'})\n",
    "    med_pd['NDC'] = med_pd['NDC'].map(lambda x: x[:4])\n",
    "    med_pd = med_pd.drop_duplicates()    \n",
    "    med_pd = med_pd.reset_index(drop=True)\n",
    "    return med_pd\n",
    "\n",
    "def filter_1000_most_pro(pro_pd):\n",
    "    pro_count = pro_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    pro_pd = pro_pd[pro_pd['ICD9_CODE'].isin(pro_count.loc[:1000, 'ICD9_CODE'])]\n",
    "    \n",
    "    return pro_pd.reset_index(drop=True)    \n",
    "\n",
    "def filter_2000_most_diag(diag_pd):\n",
    "    diag_count = diag_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    diag_pd = diag_pd[diag_pd['ICD9_CODE'].isin(diag_count.loc[:1999, 'ICD9_CODE'])]\n",
    "    \n",
    "    return diag_pd.reset_index(drop=True)\n",
    "\n",
    "def filter_300_most_med(med_pd):\n",
    "    med_count = med_pd.groupby(by=['NDC']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "    med_pd = med_pd[med_pd['NDC'].isin(med_count.loc[:299, 'NDC'])]\n",
    "    \n",
    "    return med_pd.reset_index(drop=True)\n",
    "\n",
    "def process_all():\n",
    "    # get med and diag (visit>=2)\n",
    "    med_pd = process_med()\n",
    "    med_pd = ndc2atc4(med_pd)\n",
    "#     med_pd = filter_300_most_med(med_pd)\n",
    "    \n",
    "    diag_pd = process_diag()\n",
    "    diag_pd = filter_2000_most_diag(diag_pd)\n",
    "    \n",
    "    pro_pd = process_procedure()\n",
    "#     pro_pd = filter_1000_most_pro(pro_pd)\n",
    "    \n",
    "    med_pd_key = med_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    diag_pd_key = diag_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    pro_pd_key = pro_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "    \n",
    "    combined_key = med_pd_key.merge(diag_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    combined_key = combined_key.merge(pro_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    \n",
    "    diag_pd = diag_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    med_pd = med_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    pro_pd = pro_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "\n",
    "    # flatten and merge\n",
    "    diag_pd = diag_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index()  \n",
    "    med_pd = med_pd.groupby(by=['SUBJECT_ID', 'HADM_ID'])['NDC'].unique().reset_index()\n",
    "    pro_pd = pro_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index().rename(columns={'ICD9_CODE':'PRO_CODE'})  \n",
    "    med_pd['NDC'] = med_pd['NDC'].map(lambda x: list(x))\n",
    "    pro_pd['PRO_CODE'] = pro_pd['PRO_CODE'].map(lambda x: list(x))\n",
    "    data = diag_pd.merge(med_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "    data = data.merge(pro_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n",
    "#     data['ICD9_CODE_Len'] = data['ICD9_CODE'].map(lambda x: len(x))\n",
    "    data['NDC_Len'] = data['NDC'].map(lambda x: len(x))\n",
    "    return data\n",
    "\n",
    "def statistics():\n",
    "    print('#patients ', data['SUBJECT_ID'].unique().shape)\n",
    "    print('#clinical events ', len(data))\n",
    "    \n",
    "    diag = data['ICD9_CODE'].values\n",
    "    med = data['NDC'].values\n",
    "    pro = data['PRO_CODE'].values\n",
    "    \n",
    "    unique_diag = set([j for i in diag for j in list(i)])\n",
    "    unique_med = set([j for i in med for j in list(i)])\n",
    "    unique_pro = set([j for i in pro for j in list(i)])\n",
    "    \n",
    "    print('#diagnosis ', len(unique_diag))\n",
    "    print('#med ', len(unique_med))\n",
    "    print('#procedure', len(unique_pro))\n",
    "    \n",
    "    avg_diag = 0\n",
    "    avg_med = 0\n",
    "    avg_pro = 0\n",
    "    max_diag = 0\n",
    "    max_med = 0\n",
    "    max_pro = 0\n",
    "    cnt = 0\n",
    "    max_visit = 0\n",
    "    avg_visit = 0\n",
    "\n",
    "    for subject_id in data['SUBJECT_ID'].unique():\n",
    "        item_data = data[data['SUBJECT_ID'] == subject_id]\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        visit_cnt = 0\n",
    "        for index, row in item_data.iterrows():\n",
    "            visit_cnt += 1\n",
    "            cnt += 1\n",
    "            x.extend(list(row['ICD9_CODE']))\n",
    "            y.extend(list(row['NDC']))\n",
    "            z.extend(list(row['PRO_CODE']))\n",
    "        x = set(x)\n",
    "        y = set(y)\n",
    "        z = set(z)\n",
    "        avg_diag += len(x)\n",
    "        avg_med += len(y)\n",
    "        avg_pro += len(z)\n",
    "        avg_visit += visit_cnt\n",
    "        if len(x) > max_diag:\n",
    "            max_diag = len(x)\n",
    "        if len(y) > max_med:\n",
    "            max_med = len(y) \n",
    "        if len(z) > max_pro:\n",
    "            max_pro = len(z)\n",
    "        if visit_cnt > max_visit:\n",
    "            max_visit = visit_cnt\n",
    "        \n",
    "\n",
    "        \n",
    "    print('#avg of diagnoses ', avg_diag/ cnt)\n",
    "    print('#avg of medicines ', avg_med/ cnt)\n",
    "    print('#avg of procedures ', avg_pro/ cnt)\n",
    "    print('#avg of vists ', avg_visit/ len(data['SUBJECT_ID'].unique()))\n",
    "    \n",
    "\n",
    "    print('#max of diagnoses ', max_diag)\n",
    "    print('#max of medicines ', max_med)\n",
    "    print('#max of procedures ', max_pro)\n",
    "    print('#max of visit ', max_visit)\n",
    "    \n",
    "    \n",
    "    \n",
    "data = process_all()\n",
    "statistics()\n",
    "data.to_pickle('data_final.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocaboray for Medical Codes & Save Patient Record in pickle form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1960, 153, 1432)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "class Voc(object):\n",
    "    def __init__(self):\n",
    "        self.idx2word = {}\n",
    "        self.word2idx = {}\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            if word not in self.word2idx:\n",
    "                self.idx2word[len(self.word2idx)] = word\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "                \n",
    "def create_str_token_mapping(df):\n",
    "    diag_voc = Voc()\n",
    "    med_voc = Voc()\n",
    "    pro_voc = Voc()\n",
    "    ## only for DMNC\n",
    "#     diag_voc.add_sentence(['seperator', 'decoder_point'])\n",
    "#     med_voc.add_sentence(['seperator', 'decoder_point'])\n",
    "#     pro_voc.add_sentence(['seperator', 'decoder_point'])\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        diag_voc.add_sentence(row['ICD9_CODE'])\n",
    "        med_voc.add_sentence(row['NDC'])\n",
    "        pro_voc.add_sentence(row['PRO_CODE'])\n",
    "    \n",
    "    dill.dump(obj={'diag_voc':diag_voc, 'med_voc':med_voc ,'pro_voc':pro_voc}, file=open('voc_final.pkl','wb'))\n",
    "    return diag_voc, med_voc, pro_voc\n",
    "\n",
    "def create_patient_record(df, diag_voc, med_voc, pro_voc):\n",
    "    records = [] # (patient, code_kind:3, codes)  code_kind:diag, proc, med\n",
    "    for subject_id in df['SUBJECT_ID'].unique():\n",
    "        item_df = df[df['SUBJECT_ID'] == subject_id]\n",
    "        patient = []\n",
    "        for index, row in item_df.iterrows():\n",
    "            admission = []\n",
    "            admission.append([diag_voc.word2idx[i] for i in row['ICD9_CODE']])\n",
    "            admission.append([pro_voc.word2idx[i] for i in row['PRO_CODE']])\n",
    "            admission.append([med_voc.word2idx[i] for i in row['NDC']])\n",
    "            patient.append(admission)\n",
    "        records.append(patient) \n",
    "    dill.dump(obj=records, file=open('records_final.pkl', 'wb'))\n",
    "    return records\n",
    "        \n",
    "    \n",
    "path='data_final.pkl'\n",
    "df = pd.read_pickle(path)\n",
    "diag_voc, med_voc, pro_voc = create_str_token_mapping(df)\n",
    "records = create_patient_record(df, diag_voc, med_voc, pro_voc)\n",
    "len(diag_voc.idx2word), len(med_voc.idx2word), len(pro_voc.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDI & Construct EHR Adj and DDI Adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import dill\n",
    "\n",
    "# atc -> cid\n",
    "ddi_file = 'drug-DDI.csv'\n",
    "cid_atc = 'drug-atc.csv'\n",
    "voc_file = 'voc_final.pkl'\n",
    "data_path = 'records_final.pkl'\n",
    "TOPK = 40 # topk drug-drug interaction\n",
    "\n",
    "records =  dill.load(open(data_path, 'rb'))\n",
    "cid2atc_dic = defaultdict(set)\n",
    "med_voc = dill.load(open(voc_file, 'rb'))['med_voc']\n",
    "med_voc_size = len(med_voc.idx2word)\n",
    "med_unique_word = [med_voc.idx2word[i] for i in range(med_voc_size)]\n",
    "atc3_atc4_dic = defaultdict(set)\n",
    "for item in med_unique_word:\n",
    "    atc3_atc4_dic[item[:4]].add(item)\n",
    "    \n",
    "\n",
    "with open(cid_atc, 'r') as f:\n",
    "    for line in f:\n",
    "        line_ls = line[:-1].split(',')\n",
    "        cid = line_ls[0]\n",
    "        atcs = line_ls[1:]\n",
    "        for atc in atcs:\n",
    "            if len(atc3_atc4_dic[atc[:4]]) != 0:\n",
    "                cid2atc_dic[cid].add(atc[:4])\n",
    "            \n",
    "# ddi load\n",
    "ddi_df = pd.read_csv(ddi_file)\n",
    "# fliter sever side effect \n",
    "ddi_most_pd = ddi_df.groupby(by=['Polypharmacy Side Effect', 'Side Effect Name']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n",
    "ddi_most_pd = ddi_most_pd.iloc[-TOPK:,:]\n",
    "# ddi_most_pd = pd.DataFrame(columns=['Side Effect Name'], data=['as','asd','as'])\n",
    "fliter_ddi_df = ddi_df.merge(ddi_most_pd[['Side Effect Name']], how='inner', on=['Side Effect Name'])\n",
    "ddi_df = fliter_ddi_df[['STITCH 1','STITCH 2']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# weighted ehr adj \n",
    "ehr_adj = np.zeros((med_voc_size, med_voc_size))\n",
    "for patient in records:\n",
    "    for adm in patient:\n",
    "        med_set = adm[2]\n",
    "        for i, med_i in enumerate(med_set):\n",
    "            for j, med_j in enumerate(med_set):\n",
    "                if j<=i:\n",
    "                    continue\n",
    "                ehr_adj[med_i, med_j] = 1\n",
    "                ehr_adj[med_j, med_i] = 1\n",
    "dill.dump(ehr_adj, open('ehr_adj_final.pkl', 'wb'))  \n",
    "\n",
    "\n",
    "\n",
    "# ddi adj\n",
    "ddi_adj = np.zeros((med_voc_size,med_voc_size))\n",
    "for index, row in ddi_df.iterrows():\n",
    "    # ddi\n",
    "    cid1 = row['STITCH 1']\n",
    "    cid2 = row['STITCH 2']\n",
    "    \n",
    "    # cid -> atc_level3\n",
    "    for atc_i in cid2atc_dic[cid1]:\n",
    "        for atc_j in cid2atc_dic[cid2]:\n",
    "            \n",
    "            # atc_level3 -> atc_level4\n",
    "            for i in atc3_atc4_dic[atc_i]:\n",
    "                for j in atc3_atc4_dic[atc_j]:\n",
    "                    if med_voc.word2idx[i] != med_voc.word2idx[j]:\n",
    "                        ddi_adj[med_voc.word2idx[i], med_voc.word2idx[j]] = 1\n",
    "                        ddi_adj[med_voc.word2idx[j], med_voc.word2idx[i]] = 1\n",
    "dill.dump(ddi_adj, open('ddi_A_final.pkl', 'wb')) \n",
    "                        \n",
    "print('complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
